# VLA コンペティション 評価手順書（ドラフト）

> 対象: 学部上級〜修士 ｜ 環境: LIBERO（シミュレーション） ｜ 重視: 問題分析力

---

## 1. 設計思想

### 1.1 なぜこのコンペが必要か

現行の VLA（Vision-Language-Action）モデルは、標準ベンチマーク（LIBERO）上で 90% 超の成功率を達成している。
しかし LIBERO-PRO（[arXiv:2510.03827](https://arxiv.org/abs/2510.03827)）が示したとおり、
**物体位置を 0.2 ユニットずらすだけで成功率が 0% に崩壊する**。
これは「見て理解して動いている」のではなく、**学習データの軌道を丸暗記して再生しているだけ**であることを意味する。

つまり「リーダーボードの数字を上げる」だけでは、モデルの本当の能力は測れない。

### 1.2 このコンペで育てたい能力

| 能力 | 具体的な行動 |
|------|------------|
| **問題分析力**（最重視） | なぜ失敗するのかを特定し、改善戦略を立てられる |
| 実装力 | VLA モデルの fine-tune・推論パイプラインを構築できる |
| 汎化設計力 | 丸暗記ではなく本当に汎化するモデルを設計しようとする |

### 1.3 教育的な設計原則

- **段階的な気づき**: 最初に「簡単に高スコアが出る」体験をさせ、次のフェーズで「実はそれは丸暗記だった」と気づかせる
- **プロセス重視**: 最終スコアだけでなく、分析レポートに大きな配点を置く
- **失敗が学びになる**: 摂動で崩壊する経験こそが、VLA の本質的課題を理解する入口

---

## 2. コンペティション構成（3フェーズ）

### Phase 1: Baseline（標準評価）— 2週間

**目的**: VLA パイプラインの構築を学び、「高スコアを出す」体験をする

#### 参加者がやること
1. openpi の LIBERO 環境をセットアップする
2. 提供された π0/π0-FAST の事前学習済みチェックポイントを LIBERO データで fine-tune する
3. LIBERO-Spatial / LIBERO-Object / LIBERO-Goal / LIBERO-Long の 4 スイートで評価する

#### 評価方法
- **自動評価**: 各スイート × 10 タスク × 50 エピソードの success rate
- スコア = 4 スイートの平均 success rate

#### 提出物
- fine-tune 済みチェックポイント
- 評価ログ（自動生成）
- **Phase 1 レポート**（1ページ）: 学習曲線、スイートごとの成功率の違いの考察

#### 期待値
- ほとんどのチームが 80-95% の success rate を達成する（これは想定通り）
- この段階では「高スコア＝良いモデル」と思わせる

---

### Phase 2: Robustness（摂動評価）— 2週間

**目的**: 丸暗記の限界を体験し、「なぜ崩壊するか」を分析する

#### 参加者がやること
Phase 1 のモデルをそのまま、以下の摂動条件で評価する（再学習なし）:

| 摂動軸 | 内容 | 根拠 |
|--------|------|------|
| **Position** | 物体の初期位置を ±0.1, ±0.2, ±0.3 ユニット変位 | LIBERO-PRO: 0.2 で π0 が 0% に崩壊 |
| **Instruction** | タスク指示文をパラフレーズ（同義語置換） | LIBERO-PRO: 無意味文字列でも同じ軌道を生成 |
| **Object** | 対象物体の色・テクスチャを変更 | LIBERO-PRO: 表面上は堅牢だが実は言語を無視しているだけ |
| **Composition** | 2つの既知サブタスクを合成した新指示 | LIBERO-PRO Figure 7: サブタスク合成で完全崩壊 |

#### 摂動の実装
- LIBERO-PRO（[GitHub](https://github.com/Zxy-MLlab/LIBERO-PRO)）の `perturbation.py` + `evaluation_config.yaml` を利用
- 運営側が摂動パラメータを事前設定し、参加者に Docker イメージとして配布
- 参加者は `evaluation_config.yaml` の各フラグを切り替えるだけで摂動評価を実行可能

```yaml
# 参加者が切り替える設定例
use_swap: true       # Position摂動
use_object: false    # Object摂動
use_language: false  # Semantic摂動
use_task: false      # Task摂動
use_environment: false
```

#### 評価方法
- **自動評価**（40%）: 摂動条件下の success rate
  - Position摂動スコア: 3段階の変位での平均 success rate
  - Instruction摂動スコア: パラフレーズ条件での success rate
  - Object摂動スコア: 属性変更条件での success rate
  - Composition摂動スコア: 合成タスクでの success rate

- **分析レポート**（60%）: Phase 2 の核心
  - 各摂動で何が起きたか（定量的な崩壊の記述）
  - なぜ崩壊するのかの仮説（モデルのどの部分がボトルネックか）
  - 改善戦略の提案（Phase 3 で何を試すか）
  - 3〜5ページ

#### レポート評価ルーブリック

| 項目 | 配点 | 基準 |
|------|------|------|
| **現象の正確な記述** | 15点 | 摂動ごとの崩壊パターンを定量的に示せているか |
| **原因分析の深さ** | 20点 | 表面的（「位置が変わったから」）でなく構造的原因まで踏み込めているか |
| **モデル内部の理解** | 10点 | attention / action head / vision encoder 等、アーキテクチャレベルで考察しているか |
| **改善戦略の妥当性** | 15点 | 提案がロジカルで、Phase 3 で検証可能な形になっているか |

---

### Phase 3: Improvement（改善実装）— 3週間

**目的**: 分析結果をもとに実際にモデルを改善し、汎化性能を向上させる

#### 参加者がやること
Phase 2 の分析を踏まえ、以下のいずれか（または組み合わせ）を実施:

- **データ拡張**: 学習データに位置摂動・言語摂動を追加
- **学習手法の変更**: ドメインランダマイゼーション、カリキュラム学習など
- **アーキテクチャの工夫**: vision encoder の改良、language conditioning の強化など
- **その他**: 参加者の自由な発想

#### 評価方法
- **自動評価**（50%）:
  - Phase 1 条件（標準）の success rate: 退行していないことを確認
  - Phase 2 条件（摂動）の success rate: 改善度を測定
  - 総合スコア = 0.3 × 標準SR + 0.7 × 摂動SR

- **最終レポート**（50%）:
  - 改善手法の詳細（何をなぜ試したか）
  - Phase 2 → Phase 3 での定量的改善の分析
  - 成功した手法・失敗した手法の両方の報告
  - 今後の展望
  - 5〜8ページ

#### 最終レポート評価ルーブリック

| 項目 | 配点 | 基準 |
|------|------|------|
| **手法の妥当性** | 15点 | Phase 2 の分析と論理的に整合した改善を試みているか |
| **実験の質** | 15点 | ablation があるか、複数条件で検証しているか |
| **失敗の分析** | 10点 | うまくいかなかった試みも報告し、原因を考察しているか |
| **洞察の深さ** | 10点 | VLA の本質的な課題について独自の知見を示しているか |

---

## 3. 総合スコア計算

```
最終スコア = Phase1(10%) + Phase2(45%) + Phase3(45%)
```

### Phase 1: 10%
- 自動評価スコア（標準 success rate）のみ
- ここは全チーム高スコアになるため、差がつかない設計（意図的）

### Phase 2: 45%
- 自動評価（摂動 SR）: 45% × 40% = 18%
- 分析レポート: 45% × 60% = 27%

### Phase 3: 45%
- 自動評価（改善後 SR）: 45% × 50% = 22.5%
- 最終レポート: 45% × 50% = 22.5%

### 配点のまとめ

| 評価要素 | 全体に占める割合 |
|----------|----------------|
| 自動評価（標準SR） | 10% + α |
| 自動評価（摂動SR） | 18% |
| 自動評価（改善後SR） | 22.5% |
| **分析レポート**（Phase 2） | **27%** |
| **最終レポート**（Phase 3） | **22.5%** |
| **レポート合計** | **49.5%** |

→ 全体のほぼ半分がレポート（問題分析）で決まる設計

---

## 4. 技術要件

### 4.1 環境構成

| コンポーネント | 詳細 |
|---------------|------|
| ベースライン環境 | LIBERO（MuJoCo / robosuite） |
| 摂動環境 | LIBERO-PRO（LIBERO の拡張） |
| モデル | openpi（π0 / π0-FAST） |
| 実行方法 | Docker コンテナ（運営が提供） |

### 4.2 参加者に必要な計算リソース

| 要件 | 最低 | 推奨 |
|------|------|------|
| GPU | NVIDIA GPU（VRAM 16GB+） | A100 / H100 |
| RAM | 32GB | 64GB |
| ストレージ | 50GB | 100GB |

※ 大学のクラスタや Google Colab Pro+ でも対応可能な範囲を想定

### 4.3 運営が提供するもの

1. **Docker イメージ**: openpi + LIBERO + LIBERO-PRO が統合済み
2. **ベースライン学習データ**: LIBERO の 4 スイート × 10 タスク × 50 デモ
3. **事前学習済みチェックポイント**: π0-FAST（openpi 公式）
4. **評価スクリプト**: 自動で success rate を算出し、JSON で出力
5. **摂動設定ファイル**: Phase 2 用の `evaluation_config.yaml`
6. **チュートリアル**: セットアップ〜Phase 1 の fine-tune までの手順書

### 4.4 参加者が提出するもの

| Phase | 提出物 |
|-------|--------|
| Phase 1 | チェックポイント、評価ログ、1ページレポート |
| Phase 2 | 摂動評価ログ、3-5ページ分析レポート |
| Phase 3 | 改善チェックポイント、評価ログ、5-8ページ最終レポート |

---

## 5. 評価プロトコルの妥当性（なぜこの設計が正しいか）

### 5.1 学術的根拠

この評価プロトコルは、以下の学術的知見に基づいている:

1. **LIBERO-PRO**（[arXiv:2510.03827](https://arxiv.org/abs/2510.03827)）
   - 標準 LIBERO で 90% 超のモデルが、摂動下で 0% に崩壊
   - 4 つの摂動軸（Position, Object, Instruction, Environment）で体系的に検証済み
   - π0, π0.5, OpenVLA の 3 モデルで再現

2. **LIBERO-Plus**（[arXiv:2510.13626](https://arxiv.org/abs/2510.13626)）
   - 7 次元（カメラ視点、ロボット初期状態、照明、背景テクスチャ等を追加）で検証
   - 95% → 30% 以下への崩壊を確認
   - 言語指示を完全に無視している現象を独立に確認

3. **LIBERO 本体**（NeurIPS 2023, Bo Liu et al.）
   - Spatial / Object / Goal / Long の 4 軸で汎化のボトルネックを分離評価する設計
   - VLA 研究のデファクト標準ベンチマーク

### 5.2 教育的根拠

- **構成主義的学習**: 自分で「崩壊」を体験してから原因を分析する → 受動的な講義より深い理解
- **段階的スキャフォールディング**: Phase 1（成功体験）→ Phase 2（問題発見）→ Phase 3（問題解決）
- **形成的評価**: Phase 2 のレポートフィードバックが Phase 3 の改善に直結
- **失敗の価値**: 「何が失敗したか」もレポートで配点される → 失敗を隠すインセンティブがない

### 5.3 既存コンペとの差別化

| 観点 | 既存の VLA リーダーボード | このコンペ |
|------|------------------------|-----------|
| 評価指標 | success rate のみ | success rate + 分析レポート |
| 摂動評価 | なし | LIBERO-PRO ベースの 4 軸摂動 |
| 丸暗記問題 | 看過されている | Phase 2 で直接対峙させる |
| 教育効果 | なし（ランキング競争のみ） | 段階的な気づきと成長を設計 |

---

## 6. 運営の実施手順

### 6.1 事前準備（コンペ開始 4 週間前）

- [ ] Docker イメージのビルド・テスト（openpi + LIBERO + LIBERO-PRO 統合）
- [ ] ベースラインモデルで Phase 1〜3 を通しで検証（想定スコア範囲の確認）
- [ ] 摂動パラメータの最終調整（難しすぎ / 簡単すぎのバランス）
- [ ] チュートリアル資料の作成
- [ ] 評価スクリプトの自動化（提出 → 評価 → スコア返却）
- [ ] レポート評価の rubric を審査員と合意

### 6.2 コンペ期間中

| 週 | Phase | 運営のアクション |
|----|-------|----------------|
| 1-2 | Phase 1 | セットアップサポート、Q&A 対応 |
| 2末 | — | Phase 1 スコアの暫定リーダーボード公開 |
| 3-4 | Phase 2 | 摂動設定の配布、分析のヒント提供（必要に応じ） |
| 4末 | — | Phase 2 レポートの受領、審査開始 |
| 5-7 | Phase 3 | Phase 2 レポートへのフィードバック返却（形成的評価） |
| 7末 | — | 最終提出締切 |
| 8 | — | 最終審査、結果発表 |

### 6.3 レポート審査体制

- 審査員: 最低 2 名 / チーム（ダブルブラインドでなくてよい）
- rubric に基づくスコアリング + コメント
- Phase 2 レポートは Phase 3 開始前にフィードバックを返す（これが教育効果の鍵）

---

## 7. リスクと対策

| リスク | 対策 |
|--------|------|
| GPU が足りない参加者がいる | Google Colab Pro+ or 大学クラスタの共有枠を確保 |
| LIBERO-PRO のセットアップが難しい | 統合 Docker で解決。チュートリアルも提供 |
| Phase 2 で全チーム 0% になり差がつかない | Position摂動の変位量を 3 段階にして、グラデーションをつける |
| レポート評価の主観性 | rubric を事前公開、審査員 2 名体制、スコア差が大きい場合は協議 |
| 参加者が Phase 2 を諦める | Phase 1 の配点を低くし、Phase 2-3 で巻き返し可能な設計にしている |
| チート（他チームのレポートを参照） | 分析の具体性で差が出る。自分のモデルの挙動に基づかない分析は低評価 |

---

## 付録 A: LIBERO-PRO の核心データ（根拠として引用）

### Position摂動の崩壊データ（LIBERO-PRO Table 2 より）

| モデル | 標準 LIBERO | 変位 0.2 | 変位 0.4 |
|--------|------------|----------|----------|
| OpenVLA | 0.98 | 0.00 | 0.00 |
| π0 | 0.92 | 0.00 | 0.00 |
| π0.5 | 0.97 | 0.38 | 0.00 |

### 言語指示無視の実験（LIBERO-PRO Section 4.3 より）

指示を「fdsgfdsgsd」に置き換えても軌道がほぼ同一 → **モデルは言語をまったく読んでいない**

### サブタスク合成の崩壊（LIBERO-PRO Figure 7 より）

- 「ボウルをストーブに置く」: 成功
- 「ストーブをつける」: 成功
- 「ボウルを置いてからストーブをつける」: **完全失敗**

---

## 付録 B: 技術スタック詳細

### openpi LIBERO セットアップ手順（参加者向け）

```bash
# 1. リポジトリのクローン
git clone https://github.com/Physical-Intelligence/openpi.git
cd openpi
git submodule update --init --recursive

# 2. Docker でビルド・実行（推奨）
sudo xhost +local:docker
SERVER_ARGS="--env LIBERO" docker compose -f examples/libero/compose.yml up --build

# 3. または手動セットアップ
# ポリシーサーバー起動
uv run scripts/serve_policy.py --env LIBERO

# 評価実行（別ターミナル）
python examples/libero/main.py
```

### LIBERO-PRO 摂動評価手順（参加者向け）

```bash
# 1. LIBERO-PRO セットアップ
conda create -n libero_pro python=3.8.13
pip install -r requirements.txt
pip install -e .

# 2. 摂動データのダウンロード
# Hugging Face から bddl_files, init_files をダウンロード

# 3. 摂動設定の編集
# evaluation_config.yaml の各フラグを切り替え

# 4. 評価実行
# （運営提供の統合スクリプトで実行）
```

### fine-tune 手順

```bash
# 正規化統計の計算
uv run scripts/compute_norm_stats.py --config-name pi0_fast_libero

# 学習実行
XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py --config-name pi0_fast_libero
```
